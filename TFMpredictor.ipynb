{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\basau\\\\TFMPython\\\\data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#Primero de todo es importante establecer nuestro workspace, deberemos apuntar a la carpeta donde se encuentran los archivos\n",
    "# train y test.\n",
    "os.path.abspath(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\basau\\\\TFMPython'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificamos que estamos en la carpeta correcta.\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quan_1 0\n",
      "Quan_10 811\n",
      "Quan_11 811\n",
      "Quan_12 811\n",
      "Quan_13 811\n",
      "Quan_14 811\n",
      "Quan_15 24\n",
      "Quan_16 481\n",
      "Quan_17 547\n",
      "Quan_18 1147\n",
      "Quan_19 940\n",
      "Quan_2 22\n",
      "Quan_20 1195\n",
      "Quan_21 1094\n",
      "Quan_22 1238\n",
      "Quan_26 0\n",
      "Quan_27 0\n",
      "Quan_28 0\n",
      "Quan_29 0\n",
      "Quan_3 6\n",
      "Quan_30 0\n",
      "Quan_4 135\n",
      "Quan_5 811\n",
      "Quan_6 811\n",
      "Quan_7 811\n",
      "Quan_8 811\n",
      "Quan_9 811\n",
      "Quant_22 1159\n",
      "Quant_23 1138\n",
      "Quant_24 683\n",
      "Quant_25 728\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename_train = 'data/TrainingDataset.csv'\n",
    "filename_test = 'data/TestDataset.csv'\n",
    "#usando panda importamos los dos archivos csv\n",
    "dataframe_train = pandas.read_csv(filename_train)\n",
    "dataframe_test = pandas.read_csv(filename_test)\n",
    "#los juntamos en un mismo dataframe\n",
    "dataframe = pandas.concat([dataframe_train, dataframe_test])\n",
    "\n",
    "quantitative_columns = filter(lambda s: s.startswith(\"Quan\"), dataframe.columns)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Lista de variables para mostrar en escala logaritmica:\n",
    "\n",
    "#to_log = [\"Quan_4\", \"Quan_5\", \"Quan_6\", \"Quan_7\", \"Quan_8\", \"Quan_9\", \"Quan_10\", \"Quan_11\", \"Quan_12\", \"Quan_13\", \"Quan_14\", \"Quan_15\", \"Quan_16\", \"Quan_17\", \"Quan_18\", \"Quan_19\", \"Quan_21\", \"Quan_22\", \"Quan_27\", \"Quan_28\", \"Quan_29\", \"Quant_22\", \"Quant_24\", \"Quant_25\"]\n",
    "to_log = [\"Quan_4\", \"Quan_15\", \"Quan_16\", \"Quan_17\", \"Quan_18\", \"Quan_19\", \"Quan_21\", \"Quan_22\", \"Quant_22\", \"Quant_24\", \"Quant_25\"]\n",
    "#recorremos todas las columnas para dibujar los histogramas de las variables cuantitativas\n",
    "for i, col in enumerate(quantitative_columns):\n",
    "    a = dataframe[col]\n",
    "    print col, pandas.isnull(a).sum()\n",
    "    plt.subplot(4,8,i)\n",
    "    if col in to_log:\n",
    "        a = np.log(a)\n",
    "   \n",
    "    plt.hist(a[pandas.notnull(a)].tolist(), bins=30, label=col)\n",
    "    plt.legend()\n",
    "print len(quantitative_columns)\n",
    "\n",
    "\n",
    "plt.show() # Si no estas en modo interactivo necesitaras esto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este es el dataset de entrenamiento:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      "este es el dataset de test:  [[ 1.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  1. ...,  0.  0.  0.]]\n",
      "\n",
      "Calculando columnas redundantes\n",
      "26 == 805\n",
      "30 == 232\n",
      "30 == 523\n",
      "30 == 1493\n",
      "30 == 1863\n",
      "31 == 403\n",
      "32 == 404\n",
      "37 == 1580\n",
      "38 == 1581\n",
      "39 == 1582\n",
      "40 == 1583\n",
      "41 == 1584\n",
      "42 == 1585\n",
      "43 == 1586\n",
      "44 == 1587\n",
      "48 == 1588\n",
      "49 == 1589\n",
      "50 == 1590\n",
      "51 == 1591\n",
      "52 == 1592\n",
      "53 == 1593\n",
      "54 == 1594\n",
      "55 == 1595\n",
      "56 == 1596\n",
      "57 == 1597\n",
      "58 == 1600\n",
      "59 == 1601\n",
      "60 == 1602\n",
      "61 == 1603\n",
      "62 == 1604\n",
      "63 == 1605\n",
      "66 == 1606\n",
      "67 == 1607\n",
      "68 == 1608\n",
      "69 == 1609\n",
      "70 == 1610\n",
      "71 == 1611\n",
      "72 == 1612\n",
      "73 == 1613\n",
      "74 == 1614\n",
      "75 == 1615\n",
      "76 == 1616\n",
      "77 == 1617\n",
      "78 == 1620\n",
      "79 == 1621\n",
      "80 == 1622\n",
      "81 == 1623\n",
      "98 == 415\n",
      "98 == 425\n",
      "99 == 289\n",
      "99 == 416\n",
      "99 == 426\n",
      "99 == 1505\n",
      "117 == 1327\n",
      "129 == 202\n",
      "130 == 203\n",
      "143 == 301\n",
      "144 == 302\n",
      "145 == 303\n",
      "146 == 304\n",
      "154 == 1571\n",
      "160 == 196\n",
      "161 == 197\n",
      "175 == 734\n",
      "225 == 584\n",
      "225 == 1153\n",
      "232 == 523\n",
      "232 == 1493\n",
      "232 == 1863\n",
      "249 == 1209\n",
      "251 == 638\n",
      "252 == 1404\n",
      "253 == 640\n",
      "254 == 538\n",
      "254 == 1258\n",
      "255 == 677\n",
      "255 == 1296\n",
      "261 == 966\n",
      "262 == 1288\n",
      "263 == 292\n",
      "263 == 1299\n",
      "266 == 801\n",
      "266 == 1516\n",
      "269 == 714\n",
      "269 == 1356\n",
      "270 == 716\n",
      "270 == 1359\n",
      "271 == 720\n",
      "271 == 1363\n",
      "271 == 1837\n",
      "273 == 1530\n",
      "274 == 742\n",
      "274 == 1403\n",
      "275 == 790\n",
      "275 == 1470\n",
      "276 == 746\n",
      "276 == 1411\n",
      "277 == 749\n",
      "277 == 1414\n",
      "278 == 750\n",
      "278 == 1417\n",
      "281 == 755\n",
      "283 == 767\n",
      "283 == 1440\n",
      "285 == 806\n",
      "285 == 1524\n",
      "286 == 770\n",
      "286 == 1447\n",
      "287 == 773\n",
      "287 == 1452\n",
      "288 == 778\n",
      "288 == 1455\n",
      "289 == 416\n",
      "289 == 426\n",
      "289 == 1505\n",
      "290 == 800\n",
      "290 == 1515\n",
      "292 == 1299\n",
      "306 == 1281\n",
      "384 == 672\n",
      "384 == 1286\n",
      "384 == 1845\n",
      "384 == 1891\n",
      "384 == 1924\n",
      "415 == 425\n",
      "416 == 426\n",
      "416 == 1505\n",
      "418 == 595\n",
      "418 == 1168\n",
      "422 == 1429\n",
      "426 == 1505\n",
      "444 == 674\n",
      "444 == 1521\n",
      "452 == 1062\n",
      "474 == 1010\n",
      "489 == 1031\n",
      "493 == 1036\n",
      "505 == 1055\n",
      "508 == 1058\n",
      "509 == 1059\n",
      "516 == 1068\n",
      "520 == 1075\n",
      "523 == 1493\n",
      "523 == 1863\n",
      "524 == 1484\n",
      "524 == 1928\n",
      "527 == 1083\n",
      "529 == 1085\n",
      "531 == 1201\n",
      "532 == 1100\n",
      "533 == 1199\n",
      "534 == 1368\n",
      "536 == 1088\n",
      "537 == 1089\n",
      "538 == 1258\n",
      "539 == 1463\n",
      "539 == 1869\n",
      "543 == 1095\n",
      "546 == 1422\n",
      "552 == 1107\n",
      "560 == 1120\n",
      "562 == 1124\n",
      "565 == 1127\n",
      "567 == 1132\n",
      "572 == 1140\n",
      "581 == 1150\n",
      "582 == 1332\n",
      "584 == 1153\n",
      "585 == 1156\n",
      "589 == 1159\n",
      "591 == 1162\n",
      "593 == 1164\n",
      "594 == 1167\n",
      "595 == 1168\n",
      "598 == 1172\n",
      "600 == 1177\n",
      "601 == 1180\n",
      "604 == 1185\n",
      "609 == 1192\n",
      "610 == 1193\n",
      "612 == 1196\n",
      "615 == 1200\n",
      "618 == 1208\n",
      "620 == 1211\n",
      "621 == 1212\n",
      "622 == 1213\n",
      "623 == 1215\n",
      "624 == 1216\n",
      "625 == 1217\n",
      "628 == 1223\n",
      "629 == 1224\n",
      "630 == 1226\n",
      "632 == 1229\n",
      "634 == 1231\n",
      "635 == 1234\n",
      "639 == 1239\n",
      "642 == 1242\n",
      "645 == 1250\n",
      "646 == 1251\n",
      "646 == 1750\n",
      "647 == 1252\n",
      "648 == 1190\n",
      "649 == 1255\n",
      "651 == 1256\n",
      "651 == 1862\n",
      "652 == 1257\n",
      "654 == 1260\n",
      "655 == 1261\n",
      "659 == 1266\n",
      "661 == 1269\n",
      "665 == 1385\n",
      "666 == 1275\n",
      "668 == 1278\n",
      "670 == 1282\n",
      "672 == 1286\n",
      "672 == 1845\n",
      "672 == 1891\n",
      "672 == 1924\n",
      "674 == 1521\n",
      "675 == 1289\n",
      "676 == 1291\n",
      "677 == 1296\n",
      "679 == 1298\n",
      "682 == 1304\n",
      "686 == 1312\n",
      "687 == 1315\n",
      "688 == 1372\n",
      "692 == 1319\n",
      "693 == 1321\n",
      "694 == 1322\n",
      "697 == 1333\n",
      "698 == 1335\n",
      "699 == 1337\n",
      "700 == 1338\n",
      "702 == 1346\n",
      "702 == 1772\n",
      "704 == 1349\n",
      "705 == 1350\n",
      "707 == 1352\n",
      "708 == 1453\n",
      "710 == 1353\n",
      "711 == 1354\n",
      "714 == 1356\n",
      "715 == 1357\n",
      "715 == 1561\n",
      "715 == 1563\n",
      "715 == 1565\n",
      "716 == 1359\n",
      "717 == 1360\n",
      "718 == 1361\n",
      "719 == 1362\n",
      "720 == 1363\n",
      "720 == 1837\n",
      "723 == 1370\n",
      "725 == 1374\n",
      "726 == 1375\n",
      "727 == 1376\n",
      "728 == 1377\n",
      "728 == 1718\n",
      "729 == 1379\n",
      "731 == 1387\n",
      "732 == 1388\n",
      "733 == 1390\n",
      "735 == 1394\n",
      "735 == 1867\n",
      "737 == 1395\n",
      "740 == 1400\n",
      "742 == 1403\n",
      "743 == 1405\n",
      "744 == 1407\n",
      "746 == 1411\n",
      "747 == 1412\n",
      "748 == 1413\n",
      "749 == 1414\n",
      "750 == 1417\n",
      "751 == 1418\n",
      "752 == 1309\n",
      "753 == 1419\n",
      "753 == 1929\n",
      "754 == 1423\n",
      "757 == 1461\n",
      "760 == 1430\n",
      "761 == 1895\n",
      "761 == 1940\n",
      "762 == 1432\n",
      "763 == 1433\n",
      "765 == 1438\n",
      "767 == 1440\n",
      "768 == 1444\n",
      "769 == 1445\n",
      "770 == 1447\n",
      "771 == 1448\n",
      "773 == 1452\n",
      "774 == 1536\n",
      "775 == 1553\n",
      "775 == 1557\n",
      "775 == 1559\n",
      "775 == 1566\n",
      "775 == 1770\n",
      "776 == 1178\n",
      "776 == 1889\n",
      "776 == 1920\n",
      "778 == 1455\n",
      "780 == 1456\n",
      "781 == 1457\n",
      "783 == 1460\n",
      "784 == 1462\n",
      "787 == 1466\n",
      "788 == 1468\n",
      "789 == 905\n",
      "789 == 1469\n",
      "790 == 1470\n",
      "791 == 1472\n",
      "792 == 1476\n",
      "793 == 1480\n",
      "794 == 1494\n",
      "795 == 1495\n",
      "797 == 1506\n",
      "798 == 1509\n",
      "799 == 1512\n",
      "800 == 1515\n",
      "801 == 1516\n",
      "802 == 1537\n",
      "803 == 1529\n",
      "804 == 1520\n",
      "806 == 1524\n",
      "807 == 833\n",
      "807 == 1525\n",
      "807 == 1894\n",
      "807 == 1930\n",
      "808 == 1528\n",
      "810 == 1531\n",
      "811 == 1535\n",
      "833 == 1525\n",
      "833 == 1894\n",
      "833 == 1930\n",
      "843 == 1420\n",
      "905 == 1469\n",
      "971 == 998\n",
      "971 == 1952\n",
      "972 == 1381\n",
      "977 == 1473\n",
      "997 == 1951\n",
      "998 == 1952\n",
      "1003 == 1630\n",
      "1004 == 1631\n",
      "1005 == 1632\n",
      "1006 == 1633\n",
      "1007 == 1634\n",
      "1008 == 1635\n",
      "1082 == 1844\n",
      "1096 == 1568\n",
      "1103 == 1846\n",
      "1116 == 1957\n",
      "1178 == 1889\n",
      "1178 == 1920\n",
      "1251 == 1750\n",
      "1256 == 1862\n",
      "1286 == 1845\n",
      "1286 == 1891\n",
      "1286 == 1924\n",
      "1287 == 1858\n",
      "1301 == 1664\n",
      "1306 == 1851\n",
      "1330 == 1892\n",
      "1330 == 1926\n",
      "1343 == 1756\n",
      "1346 == 1772\n",
      "1357 == 1561\n",
      "1357 == 1563\n",
      "1357 == 1565\n",
      "1363 == 1837\n",
      "1369 == 1714\n",
      "1377 == 1718\n",
      "1394 == 1867\n",
      "1398 == 1866\n",
      "1419 == 1929\n",
      "1441 == 1896\n",
      "1441 == 1932\n",
      "1449 == 1809\n",
      "1463 == 1869\n",
      "1474 == 1757\n",
      "1484 == 1928\n",
      "1490 == 1871\n",
      "1491 == 1805\n",
      "1492 == 1936\n",
      "1493 == 1863\n",
      "1502 == 1934\n",
      "1510 == 1865\n",
      "1523 == 1807\n",
      "1525 == 1894\n",
      "1525 == 1930\n",
      "1549 == 1569\n",
      "1552 == 1556\n",
      "1552 == 1558\n",
      "1552 == 1769\n",
      "1553 == 1557\n",
      "1553 == 1559\n",
      "1553 == 1566\n",
      "1553 == 1770\n",
      "1556 == 1558\n",
      "1556 == 1769\n",
      "1557 == 1559\n",
      "1557 == 1566\n",
      "1557 == 1770\n",
      "1558 == 1769\n",
      "1559 == 1566\n",
      "1559 == 1770\n",
      "1560 == 1562\n",
      "1561 == 1563\n",
      "1561 == 1565\n",
      "1563 == 1565\n",
      "1566 == 1770\n",
      "1739 == 1798\n",
      "1740 == 1799\n",
      "1789 == 1850\n",
      "1845 == 1891\n",
      "1845 == 1924\n",
      "1885 == 1912\n",
      "1888 == 1919\n",
      "1889 == 1920\n",
      "1890 == 1921\n",
      "1891 == 1924\n",
      "1892 == 1926\n",
      "1894 == 1930\n",
      "1895 == 1940\n",
      "1896 == 1932\n",
      "1897 == 1933\n",
      "1898 == 1939\n",
      "1967 == 1970\n",
      "1967 == 1971\n",
      "1970 == 1971\n",
      "1983 == 1986\n",
      "1983 == 1987\n",
      "1986 == 1987\n",
      "Saving dataset.\n",
      "Dataset saved. Everything OK\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "import gzip\n",
    "import datetime\n",
    "\n",
    "#lista de columnas con variables cuantitativas que son representativas para la escala log\n",
    "# (previo uso de explore.py)\n",
    "to_log = [\"Quan_4\", \"Quan_5\", \"Quan_6\", \"Quan_7\", \"Quan_8\", \"Quan_9\", \"Quan_10\", \"Quan_11\", \"Quan_12\", \"Quan_13\", \"Quan_14\", \"Quan_15\", \"Quan_16\", \"Quan_17\", \"Quan_18\", \"Quan_19\", \"Quan_21\", \"Quan_22\", \"Quan_27\", \"Quan_28\", \"Quan_29\", \"Quant_22\", \"Quant_24\", \"Quant_25\"]\n",
    "\n",
    "def create_dataset(dataframe_train, dataframe_test):\n",
    "    #creamos una variable local\n",
    "    global to_log\n",
    "    #unimos los dos dataframe para crear el conjunto de datos completo\n",
    "    dataframe = pandas.concat([dataframe_train, dataframe_test])\n",
    "    #computamos diferencia entre fechas\n",
    "    dataframe['Date_3'] = dataframe.Date_1 - dataframe.Date_2\n",
    "    train_size = dataframe_train.shape[0]\n",
    "    X_categorical = []\n",
    "    X_quantitative = []\n",
    "    X_date = []\n",
    "    X_id = []\n",
    "    #creamos vector de 0 para la futura predicción\n",
    "    ys = np.zeros((train_size,12), dtype=np.int)\n",
    "    columns = []\n",
    "    for col in dataframe.columns:\n",
    "        if col.startswith('Cat_'):\n",
    "            columns.append(col)\n",
    "            uni = np.unique(dataframe[col])\n",
    "            uni = uni.tolist()\n",
    "            if len(uni) > 1:\n",
    "                #binarizamos las variables categoricas\n",
    "                X_categorical.append(uni==dataframe[col].values[:,None])\n",
    "        elif col.startswith('Quan_') or col.startswith('Quant_'):\n",
    "            columns.append(col)\n",
    "            #verificamos si la columna esta en la variable to_log\n",
    "            if col in to_log:\n",
    "                dataframe[col] = np.log(dataframe[col])\n",
    "            # Si no encontramos la columna en to_log la llenamos de NaN\n",
    "            if (pandas.isnull(dataframe[col])).sum() > 1:\n",
    "                tmp = dataframe[col].copy()\n",
    "                # calculo de la mediana:\n",
    "                tmp = tmp.fillna(tmp.median())\n",
    "                X_quantitative.append(tmp.values)\n",
    "        elif col.startswith('Date_'):\n",
    "            columns.append(col)\n",
    "            # Si la columna no existe la llenamos de valores NaN:\n",
    "            tmp = dataframe[col].copy()\n",
    "            if (pandas.isnull(tmp)).sum() > 1:\n",
    "                # calculo de mediana:\n",
    "                tmp = tmp.fillna(tmp.median())\n",
    "            X_date.append(tmp.values[:,None])\n",
    "            #extraemos dia mes y año para otener efectos estacionarios de las ventas:            \n",
    "            year = np.zeros((tmp.size,1))\n",
    "            month = np.zeros((tmp.size,1))\n",
    "            day = np.zeros((tmp.size,1))\n",
    "            for i, date_number in enumerate(tmp):\n",
    "                date = datetime.date.fromordinal(int(date_number))\n",
    "                year[i,0] = date.year\n",
    "                month[i,0] = date.month\n",
    "                day[i,0] = date.day\n",
    "            X_date.append(year)\n",
    "            X_date.append(month)\n",
    "            X_date.append(day)\n",
    "            #considerando año, mes y dia como variables categoricas\n",
    "            #creamos la representacion binaria:\n",
    "            X_date.append((np.unique(year)==year).astype(np.int))\n",
    "            X_date.append((np.unique(month)==month).astype(np.int))\n",
    "            X_date.append((np.unique(day)==day).astype(np.int))\n",
    "        elif col=='id':\n",
    "            pass # X_id.append(dataframe[col].values)\n",
    "        elif col.startswith('Outcome_'):\n",
    "            outcome_col_number = int(col.split('M')[1]) - 1\n",
    "            tmp = dataframe[col][:train_size].copy()\n",
    "            # calculo de mediana:\n",
    "            tmp = tmp.fillna(tmp.median())\n",
    "            ys[:,outcome_col_number] = tmp.values\n",
    "        else:\n",
    "            raise NameError\n",
    "\n",
    "    X_categorical = np.hstack(X_categorical).astype(np.float32)\n",
    "    X_quantitative = np.vstack(X_quantitative).astype(np.float32).T\n",
    "    X_date = np.hstack(X_date).astype(np.float32)\n",
    "\n",
    "    X = np.hstack([X_categorical, X_quantitative, X_date])\n",
    "    X_train = X[:train_size,:]\n",
    "    X_test = X[train_size:,:]\n",
    "    return X_train, X_test, ys, columns\n",
    "\n",
    "\n",
    "def redundant_columns(X):\n",
    "    \"\"\"Identificar columnas redundantes.\n",
    "    \"\"\"\n",
    "    idx = []\n",
    "    for i in range(X.shape[1]-1):\n",
    "        for j in range(i+1, X.shape[1]):\n",
    "            if (X[:,i] == X[:,j]).all() :\n",
    "                print i, '==', j\n",
    "                idx.append(j)\n",
    "    return np.unique(idx)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    filename_train = 'data/TrainingDataset.csv'\n",
    "    filename_test = 'data/TestDataset.csv'\n",
    "    dataframe_train = pandas.read_csv(filename_train)\n",
    "    dataframe_test = pandas.read_csv(filename_test)\n",
    "    # Hay que tener en cuenta que el dataframe tiene las columnas en diferente\n",
    "    # orden que dataframe_train y dataframe_test\n",
    "    \n",
    "    \"\"\"print \"dataframe_train:\", dataframe_train\n",
    "    print\n",
    "    print \"dataframe_test:\", dataframe_test\n",
    "    \"\"\"\n",
    "    ids = dataframe_test.values[:,0].astype(np.int)\n",
    "\n",
    "    X_train, X_test, ys, columns = create_dataset(dataframe_train, dataframe_test)\n",
    "    \n",
    "    print \"Este es el dataset de entrenamiento: \", X_train\n",
    "    print\n",
    "    print \"este es el dataset de test: \", X_test\n",
    "    \n",
    "    print\n",
    "    print \"Calculando columnas redundantes\"\n",
    "    X = np.vstack([X_train, X_test])\n",
    "    idx = redundant_columns(X)\n",
    "    columns_to_keep = list(set(range(X.shape[1])).difference(set(idx.tolist())))\n",
    "    X = X[:,columns_to_keep]\n",
    "    X_train = X[:X_train.shape[0], :]\n",
    "    X_test = X[X_train.shape[0]:, :]\n",
    "    \n",
    "    print \"Saving dataset.\"\n",
    "    all_data = {\"X_train\": X_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"columns\": columns,\n",
    "                \"ys\": ys,\n",
    "                \"ids\": ids,\n",
    "                \"redundant\": idx}\n",
    "    pickle.dump(all_data, gzip.open('all_data.pickle.gz','w'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Dataset saved. Everything OK\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 0\n",
      "Fold 0\n",
      "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.001,\n",
      "             loss='ls', max_depth=6, max_features=None,\n",
      "             max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             random_state=None, subsample=0.5, verbose=0, warm_start=False)\n",
      "y tiene valores infinitos:  False\n",
      "y tiene valores nan:  False\n",
      "X tiene valores nan:  False\n",
      "X tiene valores infinitos:  False\n",
      "Fold 1\n",
      "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.001,\n",
      "             loss='ls', max_depth=6, max_features=None,\n",
      "             max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             random_state=None, subsample=0.5, verbose=0, warm_start=False)\n",
      "y tiene valores infinitos:  False\n",
      "y tiene valores nan:  False\n",
      "X tiene valores nan:  False\n",
      "X tiene valores infinitos:  False\n",
      "Fold 2\n",
      "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.001,\n",
      "             loss='ls', max_depth=6, max_features=None,\n",
      "             max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             random_state=None, subsample=0.5, verbose=0, warm_start=False)\n",
      "y tiene valores infinitos:  False\n",
      "y tiene valores nan:  False\n",
      "X tiene valores nan:  False\n",
      "X tiene valores infinitos:  False\n",
      "Month 1\n",
      "Fold 0\n",
      "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.001,\n",
      "             loss='ls', max_depth=6, max_features=None,\n",
      "             max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             random_state=None, subsample=0.5, verbose=0, warm_start=False)\n",
      "y tiene valores infinitos: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple blender para los valores de regresion deseados durante meses\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "import load_data\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LinearRegression\n",
    "import pickle\n",
    "import gzip\n",
    "import math\n",
    "\n",
    "def rmsle_loop(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #iniciamos la seed para la aleatoriedad y creamos un 5 fold cross validation\n",
    "\n",
    "    np.random.seed(0)\n",
    "    n_folds = 3\n",
    "    \n",
    "    #cagamos el dataset\n",
    "\n",
    "    X, X_submission, ys, ids, idx = load_data.load()    \n",
    "    \n",
    "    # evitamos el logscale en la evaluacion:\n",
    "    ys = np.log(ys/500.0 + 1.0)      \n",
    "    y_submission = np.zeros((X_submission.shape[0], 12))    \n",
    "\n",
    "    # regs = [RandomForestRegressor(n_estimators=100, n_jobs=-1, max_features='auto'),\n",
    "    #         ExtraTreesRegressor(n_estimators=100, n_jobs=-1, max_features='auto'),\n",
    "    #         GradientBoostingRegressor(learning_rate=0.001, subsample=0.5, max_depth=6, n_estimators=20000)]\n",
    "\n",
    "    #se prueba con n stimators 1000 para que se ejecute más rápido\n",
    "    regs = [GradientBoostingRegressor(learning_rate=0.001, subsample=0.5, max_depth=6, n_estimators=1000)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((X.shape[0], 12*len(regs)), dtype=np.double)\n",
    "    dataset_blend_submission = np.zeros((X_submission.shape[0], 12*len(regs), n_folds), dtype=np.double)\n",
    "    \n",
    "    \n",
    "    for i in range(12):\n",
    "        print \"Month\", i\n",
    "        y = ys[:,i]\n",
    "        kfcv = KFold(n=X.shape[0], n_folds=n_folds)\n",
    "        for j, (train, test) in enumerate(kfcv):\n",
    "            print \"Fold\", j\n",
    "            for k, reg in enumerate(regs):\n",
    "                print reg\n",
    "                #Nos aseguramos de eliminar todos los valores infinitos o NaN\n",
    "                y[train] = np.nan_to_num(y[train])\n",
    "                X[train] = np.nan_to_num(X[train])\n",
    "                X[test] = np.nan_to_num(X[test])\n",
    "                X_submission = np.nan_to_num(X_submission)\n",
    "                #check de valores NaN o infinitos\n",
    "                print \"y tiene valores infinitos: \", np.isinf(y[train]).any()\n",
    "                print \"y tiene valores nan: \", np.isnan(y[train]).any()\n",
    "                print \"X tiene valores nan: \", np.isnan(X[train]).any()\n",
    "                print \"X tiene valores infinitos: \", np.isnan(X[train]).any()                \n",
    "                reg.fit(X[train], y[train])\n",
    "                #ejecutamos el predictor\n",
    "                dataset_blend_train[test,12*k+i] = reg.predict(X[test])\n",
    "                dataset_blend_submission[:,12*k+i,j] = reg.predict(X_submission)\n",
    "\n",
    "    \n",
    "    dataset_blend_submission_final = dataset_blend_submission.mean(2)\n",
    "    print \"dataset_blend_submission_final:\", dataset_blend_submission_final.shape\n",
    "\n",
    "    print \"Blending.\"\n",
    "    for i in range(12):\n",
    "        print \"Month\", i, '-',\n",
    "        y = ys[:,i]\n",
    "        reg = RidgeCV(alphas=np.logspace(-2,4,40))\n",
    "        reg.fit(dataset_blend_train, y)\n",
    "        print \"best_alpha =\", reg.alpha_\n",
    "        y_submission[:,i] = reg.predict(dataset_blend_submission_final)\n",
    "                \n",
    "    # reconversion de los resultados a la dimension original:\n",
    "    y_submission = (np.exp(y_submission) - 1.0) * 500.0\n",
    "    \n",
    "    print \"Guardando resultados en test.csv...\"\n",
    "    np.savetxt(\"test.csv\", np.hstack([ids[:,None], y_submission]), fmt=\"%d\", delimiter=',')\n",
    "    print(\"Resultados guardados en test.csv\")\n",
    "    ys = (np.exp(ys) - 1.0) * 500.0\n",
    "    print rmsle_loop(ys, y_submission)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Un cargados simple del conjunto de datos y que mezcla las lineas para crear \n",
    "un conjunto de datos aleatorio\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def load(filename='all_data.pickle.gz', shuffle_train=False):\n",
    "    \"\"\"Load dataset. Shuffle train data if requested\n",
    "    \"\"\"\n",
    "    f = gzip.open(filename)\n",
    "    all_data = pickle.load(f)\n",
    "    X_train = all_data['X_train']\n",
    "    X_test = all_data['X_test']\n",
    "    ys = all_data['ys']\n",
    "    ids = all_data['ids']\n",
    "    idx = np.arange(X_train.shape[0])\n",
    "    if shuffle_train:\n",
    "        idx = np.random.permutation(idx)\n",
    "        X_train = X_train[idx, :]\n",
    "        ys = ys[idx, :]\n",
    "    return X_train, X_test, ys, ids, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
